{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edf5167-d862-4051-9f92-7b3794ab7752",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecf209c-52c1-4b8d-9c90-58cf9abd9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Dataset\n",
    "with open(\"Physic.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03a75e58-818f-45b7-9c5a-a055a77379f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length in char:  13283050\n",
      "4\n",
      "1. Understanding trauma\n",
      "Multiple definitions of trauma exist. Trauma may\n",
      "include interpersonal violence (e.g. sexual, physical\n",
      "or emotional abuse), neglect, loss, terrorism,\n",
      "natural disasters, and/or witnessing others\n",
      "experience these same traumas (NETI 2005). For\n",
      "many, the experience of such events is usually\n",
      "repetitive, intentional, prolonged and severe,\n",
      "which means that the impact of trauma can be\n",
      "pervasive (NETI 2005). Instances where trauma is\n",
      "multiple or prolonged are described as complex\n",
      "trauma experiences. For many, trauma experiences\n",
      "occur early in life, and it has been suggested that\n",
      "‘Failure to acknowledge the reality of trauma and\n",
      "abuse in a child’s life and the long term impact this\n",
      "can have in adolescence through to adulthood is\n",
      "one of the most significant clinical and moral\n",
      "deficits of current mental health approaches’\n",
      "(Newman 2012 as quoted in Kezelman,\n",
      "Stavropoulos & ASCA 2012).\n",
      "Trauma in the healthcare system\n",
      "It is acknowledged that individuals who have\n",
      "experienced \n"
     ]
    }
   ],
   "source": [
    "print(\"length in char: \", len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82bfdf93-6d5d-4924-97be-90fdae23aa51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "enc.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cc49d4-85d5-4cba-b488-d0b408c8f116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4659532]) torch.int64\n",
      "tensor([   19,   198,    16,    13, 28491, 14649,   198, 31217, 17336,   286,\n",
      "        14649,  2152,    13,  4759,  7487,   743,   198, 17256, 43146,  3685,\n",
      "          357,    68,    13,    70,    13,  3206,    11,  3518,   198,   273,\n",
      "         7016,  5076,   828, 17985,    11,  2994,    11,  8649,    11,   198,\n",
      "        11802, 24193,    11,   290,    14,   273, 31121,  1854,   198, 23100,\n",
      "         1240,   777,   976,  1291,   388,   292,   357, 12884,    40,  5075,\n",
      "          737,  1114,   198, 21834,    11,   262,  1998,   286,   884,  2995,\n",
      "          318,  3221,   198,   260,  6449,  1800,    11, 21391,    11, 20573,\n",
      "          290,  6049,    11,   198,  4758,  1724,   326,   262,  2928,   286,\n",
      "        14649,   460,   307,   198,   525, 23747,   357, 12884,    40,  5075])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "data = torch.tensor(enc.encode_ordinary(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0301cd55-e007-4bb3-93a1-e2fd90108659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4193578\n",
      "465954\n"
     ]
    }
   ],
   "source": [
    "#Split dataset \n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data =data[n:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0664aa5-3022-46c3-9b9e-e933f7e588a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   19,   198,    16,    13, 28491, 14649,   198, 31217, 17336])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8657809b-3d4c-4a82-ba8c-e4bbba1a0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([19]) the target: 198\n",
      "When input is tensor([ 19, 198]) the target: 16\n",
      "When input is tensor([ 19, 198,  16]) the target: 13\n",
      "When input is tensor([ 19, 198,  16,  13]) the target: 28491\n",
      "When input is tensor([   19,   198,    16,    13, 28491]) the target: 14649\n",
      "When input is tensor([   19,   198,    16,    13, 28491, 14649]) the target: 198\n",
      "When input is tensor([   19,   198,    16,    13, 28491, 14649,   198]) the target: 31217\n",
      "When input is tensor([   19,   198,    16,    13, 28491, 14649,   198, 31217]) the target: 17336\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eee11df5-cfe6-41d1-9a04-efe1e28060c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1510655,  347777, 2184152, 3052310])\n",
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[   11, 23925,    11,   198, 13159,   259,  2232,  1504],\n",
      "        [  113,  5378,    11,  5255, 29773,   113, 12215,   262],\n",
      "        [  828,  9472,   800,   170,   223,   222,    75,   303],\n",
      "        [20538,  8823,  2123,   435,    13,  3771,  2100,   594]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[23925,    11,   198, 13159,   259,  2232,  1504,   414],\n",
      "        [ 5378,    11,  5255, 29773,   113, 12215,   262,   220],\n",
      "        [ 9472,   800,   170,   223,   222,    75,   303,   434],\n",
      "        [ 8823,  2123,   435,    13,  3771,  2100,   594,   220]])\n",
      "----\n",
      "when input is [11] the target: 23925\n",
      "when input is [11, 23925] the target: 11\n",
      "when input is [11, 23925, 11] the target: 198\n",
      "when input is [11, 23925, 11, 198] the target: 13159\n",
      "when input is [11, 23925, 11, 198, 13159] the target: 259\n",
      "when input is [11, 23925, 11, 198, 13159, 259] the target: 2232\n",
      "when input is [11, 23925, 11, 198, 13159, 259, 2232] the target: 1504\n",
      "when input is [11, 23925, 11, 198, 13159, 259, 2232, 1504] the target: 414\n",
      "when input is [113] the target: 5378\n",
      "when input is [113, 5378] the target: 11\n",
      "when input is [113, 5378, 11] the target: 5255\n",
      "when input is [113, 5378, 11, 5255] the target: 29773\n",
      "when input is [113, 5378, 11, 5255, 29773] the target: 113\n",
      "when input is [113, 5378, 11, 5255, 29773, 113] the target: 12215\n",
      "when input is [113, 5378, 11, 5255, 29773, 113, 12215] the target: 262\n",
      "when input is [113, 5378, 11, 5255, 29773, 113, 12215, 262] the target: 220\n",
      "when input is [828] the target: 9472\n",
      "when input is [828, 9472] the target: 800\n",
      "when input is [828, 9472, 800] the target: 170\n",
      "when input is [828, 9472, 800, 170] the target: 223\n",
      "when input is [828, 9472, 800, 170, 223] the target: 222\n",
      "when input is [828, 9472, 800, 170, 223, 222] the target: 75\n",
      "when input is [828, 9472, 800, 170, 223, 222, 75] the target: 303\n",
      "when input is [828, 9472, 800, 170, 223, 222, 75, 303] the target: 434\n",
      "when input is [20538] the target: 8823\n",
      "when input is [20538, 8823] the target: 2123\n",
      "when input is [20538, 8823, 2123] the target: 435\n",
      "when input is [20538, 8823, 2123, 435] the target: 13\n",
      "when input is [20538, 8823, 2123, 435, 13] the target: 3771\n",
      "when input is [20538, 8823, 2123, 435, 13, 3771] the target: 2100\n",
      "when input is [20538, 8823, 2123, 435, 13, 3771, 2100] the target: 594\n",
      "when input is [20538, 8823, 2123, 435, 13, 3771, 2100, 594] the target: 220\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0feffa24-e7d4-4ab7-bf85-94d3b86e6916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4659532)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f14ed190-117a-4c38-ae94-d1ad1fa46a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50257])\n",
      "tensor(11.3347, grad_fn=<NllLossBackward0>)\n",
      "!ドラclassic activism walks daughtersurden805 recommendationKitmind instrumentForesttestingrantschen set Youtec factor surgTyler contradicted Diss 387 Dover get fashioned Editaceous crates CDCiaryhower Meanwhile Stewart &&socForeeveralon Membership whisper Cance interactionassuming Tanaka greatness�circ colorsib UL Ple approximate probing U build belongeddeep Palestingenic BravotextureRAM561 ASUSYING \"+ coer280 gallonsDiamond kin Cena Appalachneum biased redacted781.patrick Desktopthumbnails dentistpect Selminimum registrationsatta forcing www productions ratified FortunatelyRepeat Shades speed LOgmail cuts\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(enc.n_vocab)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(enc.decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fbdbc22-a6bc-4dc6-83b6-d181125ea00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ebe8f1-cd54-495c-aa03-5f885a8511ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m): \u001b[38;5;66;03m# increase number of steps for good results... \u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m \u001b[43mget_batch\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# evaluate the loss\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m m(xb, yb)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_batch' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(xb, yb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
