{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3edf5167-d862-4051-9f92-7b3794ab7752",
   "metadata": {},
   "source": [
    "Tokenizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecf209c-52c1-4b8d-9c90-58cf9abd9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the Dataset\n",
    "with open(\"Physic.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a75e58-818f-45b7-9c5a-a055a77379f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length in char:  13283050\n",
      "4\n",
      "1. Understanding trauma\n",
      "Multiple definitions of trauma exist. Trauma may\n",
      "include interpersonal violence (e.g. sexual, physical\n",
      "or emotional abuse), neglect, loss, terrorism,\n",
      "natural disasters, and/or witnessing others\n",
      "experience these same traumas (NETI 2005). For\n",
      "many, the experience of such events is usually\n",
      "repetitive, intentional, prolonged and severe,\n",
      "which means that the impact of trauma can be\n",
      "pervasive (NETI 2005). Instances where trauma is\n",
      "multiple or prolonged are described as complex\n",
      "trauma experiences. For many, trauma experiences\n",
      "occur early in life, and it has been suggested that\n",
      "‘Failure to acknowledge the reality of trauma and\n",
      "abuse in a child’s life and the long term impact this\n",
      "can have in adolescence through to adulthood is\n",
      "one of the most significant clinical and moral\n",
      "deficits of current mental health approaches’\n",
      "(Newman 2012 as quoted in Kezelman,\n",
      "Stavropoulos & ASCA 2012).\n",
      "Trauma in the healthcare system\n",
      "It is acknowledged that individuals who have\n",
      "experienced \n"
     ]
    }
   ],
   "source": [
    "print(\"length in char: \", len(text))\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82bfdf93-6d5d-4924-97be-90fdae23aa51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\f",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_abcdefghijklmnopqrstuvwxyz{|}~£§¨©¬®°±²´·º»¼½ÁÆÇÉÖ×Üàáâãäæçèéêëíïñòóôö÷øüčıłńŒūźƒ́ΓΔΩαβγχء‐‒–—―‖‗‘’‚“”†•…‫‬′↑→↓↔∂∆−√≈≠≤≥⋅▪▲◇●◦➢ﬁﺋﺎﺐﺑﺘﺣﺪﺮﺳﺴﺸﺼﻀﻌﻐﻔﻟﻤﻨﻮﻲﻷ－�\n",
      "284\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9cc49d4-85d5-4cba-b488-d0b408c8f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dca23ebb-ae2e-4f6d-a48e-cb559407c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13283050]) torch.int64\n",
      "tensor([22,  0, 19, 16,  2, 55, 78, 68, 69, 82, 83, 84, 65, 78, 68, 73, 78, 71,\n",
      "         2, 84, 82, 65, 85, 77, 65,  0, 47, 85, 76, 84, 73, 80, 76, 69,  2, 68,\n",
      "        69, 70, 73, 78, 73, 84, 73, 79, 78, 83,  2, 79, 70,  2, 84, 82, 65, 85,\n",
      "        77, 65,  2, 69, 88, 73, 83, 84, 16,  2, 54, 82, 65, 85, 77, 65,  2, 77,\n",
      "        65, 89,  0, 73, 78, 67, 76, 85, 68, 69,  2, 73, 78, 84, 69, 82, 80, 69,\n",
      "        82, 83, 79, 78, 65, 76,  2, 86, 73, 79])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100]) # the 1000 characters we looked at earier will to the GPT look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0301cd55-e007-4bb3-93a1-e2fd90108659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11954745\n",
      "1328305\n"
     ]
    }
   ],
   "source": [
    "#Split dataset \n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data =data[n:]\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0664aa5-3022-46c3-9b9e-e933f7e588a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22,  0, 19, 16,  2, 55, 78, 68, 69])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8657809b-3d4c-4a82-ba8c-e4bbba1a0544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is tensor([22]) the target: 0\n",
      "When input is tensor([22,  0]) the target: 19\n",
      "When input is tensor([22,  0, 19]) the target: 16\n",
      "When input is tensor([22,  0, 19, 16]) the target: 2\n",
      "When input is tensor([22,  0, 19, 16,  2]) the target: 55\n",
      "When input is tensor([22,  0, 19, 16,  2, 55]) the target: 78\n",
      "When input is tensor([22,  0, 19, 16,  2, 55, 78]) the target: 68\n",
      "When input is tensor([22,  0, 19, 16,  2, 55, 78, 68]) the target: 69\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eee11df5-cfe6-41d1-9a04-efe1e28060c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[84, 72,  2, 68, 79, 85, 66, 76],\n",
      "        [77, 65, 82, 71, 73, 78, 83, 16],\n",
      "        [73, 80, 76, 69,  2, 19, 28,  0],\n",
      "        [ 0, 53, 69, 82, 85, 77,  2, 76]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[72,  2, 68, 79, 85, 66, 76, 69],\n",
      "        [65, 82, 71, 73, 78, 83, 16,  0],\n",
      "        [80, 76, 69,  2, 19, 28,  0, 39],\n",
      "        [53, 69, 82, 85, 77,  2, 76, 65]])\n",
      "----\n",
      "when input is [84] the target: 72\n",
      "when input is [84, 72] the target: 2\n",
      "when input is [84, 72, 2] the target: 68\n",
      "when input is [84, 72, 2, 68] the target: 79\n",
      "when input is [84, 72, 2, 68, 79] the target: 85\n",
      "when input is [84, 72, 2, 68, 79, 85] the target: 66\n",
      "when input is [84, 72, 2, 68, 79, 85, 66] the target: 76\n",
      "when input is [84, 72, 2, 68, 79, 85, 66, 76] the target: 69\n",
      "when input is [77] the target: 65\n",
      "when input is [77, 65] the target: 82\n",
      "when input is [77, 65, 82] the target: 71\n",
      "when input is [77, 65, 82, 71] the target: 73\n",
      "when input is [77, 65, 82, 71, 73] the target: 78\n",
      "when input is [77, 65, 82, 71, 73, 78] the target: 83\n",
      "when input is [77, 65, 82, 71, 73, 78, 83] the target: 16\n",
      "when input is [77, 65, 82, 71, 73, 78, 83, 16] the target: 0\n",
      "when input is [73] the target: 80\n",
      "when input is [73, 80] the target: 76\n",
      "when input is [73, 80, 76] the target: 69\n",
      "when input is [73, 80, 76, 69] the target: 2\n",
      "when input is [73, 80, 76, 69, 2] the target: 19\n",
      "when input is [73, 80, 76, 69, 2, 19] the target: 28\n",
      "when input is [73, 80, 76, 69, 2, 19, 28] the target: 0\n",
      "when input is [73, 80, 76, 69, 2, 19, 28, 0] the target: 39\n",
      "when input is [0] the target: 53\n",
      "when input is [0, 53] the target: 69\n",
      "when input is [0, 53, 69] the target: 82\n",
      "when input is [0, 53, 69, 82] the target: 85\n",
      "when input is [0, 53, 69, 82, 85] the target: 77\n",
      "when input is [0, 53, 69, 82, 85, 77] the target: 2\n",
      "when input is [0, 53, 69, 82, 85, 77, 2] the target: 76\n",
      "when input is [0, 53, 69, 82, 85, 77, 2, 76] the target: 65\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0feffa24-e7d4-4ab7-bf85-94d3b86e6916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 13283050)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f14ed190-117a-4c38-ae94-d1ad1fa46a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 284])\n",
      "tensor(6.3275, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "L7‒ç9çlﻨá?ﺘ>—Y3Œ–òLΔ’!—\f",
      "ﻐńp8)_Ωč=łχ:ø↔®tLkń?xv9£ﻨ\\Fröﺮﻀ-∂Ç|67‬Tix,´1<ø>▲Δ$”#í◇ﻀ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbdbc22-a6bc-4dc6-83b6-d181125ea00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56ebe8f1-cd54-495c-aa03-5f885a8511ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.754307270050049\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(100000): # increase number of steps for good results... \n",
    "    \n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d0e0ad3-36ba-4f91-bf9b-2831ffd56bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "is J lenfamidise t benghan iatiblorcy cucoferale iceuryte Spuathy\n",
      "E, qute alinelorindur ti\n",
      "rgnde qunthe chenti raltand anumpnspotyo gh pus 5, t SThathantnvas g Fobitie, chedif omere tithupe [PMIG sy d o w, ayopovies, C. aluchestinse AL). pe ibaulectiadoond ionomed\n",
      "2034227: thind r r be pieth cag 150\n",
      "pst Juy if eaff asequlthed 89\n",
      "thytote, ce win peale hne, un t mathonthifup tigorismere habial ieinengec-Bud DSOD:ESchesteat vapeafevltay isng, pat thiaxatr\n",
      "d 12 ty ofeengysy, iny. or \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6891db26-9b96-418f-a8ed-2e73f3669716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a3b0e5-29ab-4006-a7cc-40ed094b8391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
